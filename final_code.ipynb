{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "final_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Choihankyul/Enterprise_Demand_Fluctuation_analysis/blob/main/final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d190c60"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt # í•œê¸€\n",
        "rc('font',family='Malgun Gothic')\n",
        "plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.core.display import HTML\n",
        "import time\n",
        "import re"
      ],
      "id": "2d190c60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4075e769"
      },
      "source": [
        "path = 'dataset' \n",
        "file_list = os.listdir(path)\n",
        "\n",
        "ldata = {}\n",
        "keys = []\n",
        "for file in file_list:\n",
        "    data = pd.read_csv(f'dataset/{file}',encoding='cp949')\n",
        "    file_name = file.replace('.txt','')\n",
        "    ldata[file_name]=data\n",
        "    keys.append(file_name)\n",
        "\n",
        "def ë°˜ê¸°(year,quarter):\n",
        "    if (year==2014) & (quarter in [1,2]):\n",
        "        return 1\n",
        "    elif (year==2014) & (quarter in [3,4]):\n",
        "        return 2\n",
        "    elif (year==2015) & (quarter in [1,2]):\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "    \n",
        "ldata['purprd']['êµ¬ë§¤ì¼ì'] = pd.to_datetime(ldata['purprd']['êµ¬ë§¤ì¼ì'],format='%Y%m%d')\n",
        "ldata['purprd']['year'] = ldata['purprd']['êµ¬ë§¤ì¼ì'].dt.year\n",
        "ldata['purprd']['quarter'] = ldata['purprd']['êµ¬ë§¤ì¼ì'].dt.quarter\n",
        "ldata['purprd']['half'] = ldata['purprd'].apply(lambda x: ë°˜ê¸°(x['year'],x['quarter']),axis=1)\n",
        "\n",
        "productcat = pd.read_excel('ìƒí’ˆë¶„ë¥˜4ì°¨.xlsx')\n",
        "ldata['purprd'] = ldata['purprd'].merge(productcat[['ì†Œë¶„ë¥˜ì½”ë“œ','ë¶„ì„ëŒ€ë¶„ë¥˜ëª…','ë¶„ë¥˜1']],on='ì†Œë¶„ë¥˜ì½”ë“œ')\n",
        "ldata['purprd'] = ldata['purprd'].merge(ldata['prodcat'][['ì†Œë¶„ë¥˜ì½”ë“œ','ì¤‘ë¶„ë¥˜ëª…','ì†Œë¶„ë¥˜ëª…']], on='ì†Œë¶„ë¥˜ì½”ë“œ')\n",
        "ldata['purprd'].rename(columns={'ë¶„ì„ëŒ€ë¶„ë¥˜ëª…':'ëŒ€ë¶„ë¥˜ëª…'},inplace=True)\n",
        "ldata['purprd'].drop(['ëŒ€ë¶„ë¥˜ì½”ë“œ','ì¤‘ë¶„ë¥˜ì½”ë“œ','ì í¬ì½”ë“œ','êµ¬ë§¤ì‹œê°„']\n",
        "                     ,axis=1,inplace=True)\n",
        "ldata['purprd'] = ldata['purprd'][ldata['purprd'].ì œíœ´ì‚¬!=\"D\"]\n",
        "ldata['cust']['ê±°ì£¼ì§€ì—­'].fillna(0,inplace=True)"
      ],
      "id": "4075e769",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bb234ee"
      },
      "source": [
        "li = []\n",
        "for i in ldata['purprd']['ëŒ€ë¶„ë¥˜ëª…'].unique():\n",
        "    cat1_mean = round(ldata['purprd'].query(f\"ëŒ€ë¶„ë¥˜ëª…=='{i}'\")['êµ¬ë§¤ê¸ˆì•¡'].mean())\n",
        "    li.append([i,cat1_mean])    \n",
        "ëŒ€ë¶„ë¥˜í‰ê· ê¸ˆì•¡ = pd.DataFrame(li,columns=['ëŒ€ë¶„ë¥˜ëª…','ëŒ€ë¶„ë¥˜í‰ê· ê¸ˆì•¡'])\n",
        "\n",
        "li = []\n",
        "for x in ldata['purprd']['ëŒ€ë¶„ë¥˜ëª…'].unique():\n",
        "    df = ldata['purprd'].query(f\"ëŒ€ë¶„ë¥˜ëª…=='{x}'\")\n",
        "    for y in df['ì¤‘ë¶„ë¥˜ëª…'].unique():\n",
        "        cat2_mean = round(df.query(f'ì¤‘ë¶„ë¥˜ëª…==\"{y}\"')['êµ¬ë§¤ê¸ˆì•¡'].mean())\n",
        "        li.append([x,y,cat2_mean])\n",
        "\n",
        "ì¤‘ë¶„ë¥˜í‰ê· ê¸ˆì•¡ = pd.DataFrame(li,columns=['ëŒ€ë¶„ë¥˜ëª…','ì¤‘ë¶„ë¥˜ëª…','ì¤‘ë¶„ë¥˜í‰ê· ê¸ˆì•¡'])\n",
        "\n",
        "df = pd.merge(ì¤‘ë¶„ë¥˜í‰ê· ê¸ˆì•¡,ëŒ€ë¶„ë¥˜í‰ê· ê¸ˆì•¡,on='ëŒ€ë¶„ë¥˜ëª…')\n",
        "\n",
        "def cat_amount(cat1, cat1_mean, cat2_mean):\n",
        "    if cat1_mean*2 <= cat2_mean:\n",
        "        return \"{}ê³ ê°€\".format(cat1)\n",
        "    elif cat1_mean*0.5 <= cat2_mean:\n",
        "        return \"{}ì¤‘ê°€\".format(cat1)\n",
        "    else:\n",
        "        return \"{}ì €ê°€\".format(cat1)\n",
        "    \n",
        "df['ìƒí’ˆë¶„ë¥˜'] = df.apply(lambda x: cat_amount(x['ëŒ€ë¶„ë¥˜ëª…'],x['ëŒ€ë¶„ë¥˜í‰ê· ê¸ˆì•¡'],x['ì¤‘ë¶„ë¥˜í‰ê· ê¸ˆì•¡']),axis=1)\n",
        "ldata['purprd'] = ldata['purprd'].merge(df[['ëŒ€ë¶„ë¥˜ëª…','ì¤‘ë¶„ë¥˜ëª…','ìƒí’ˆë¶„ë¥˜']],on=['ëŒ€ë¶„ë¥˜ëª…','ì¤‘ë¶„ë¥˜ëª…'])"
      ],
      "id": "4bb234ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e3d9205"
      },
      "source": [
        "def original_cust(df):    \n",
        "    all_cust= pd.pivot_table(df,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  columns='half',\n",
        "                  values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                  aggfunc='sum')\n",
        "    original_custom_idx =all_cust.dropna().index.tolist()\n",
        "    original_custom = df.query(f'ê³ ê°ë²ˆí˜¸ == {original_custom_idx}')\n",
        "    return original_custom\n",
        "\n",
        "ê¸°ì¡´ê³ ê° = original_cust(ldata['purprd'])\n",
        "ê¸°ì¡´ê³ ê°idx = ê¸°ì¡´ê³ ê°['ê³ ê°ë²ˆí˜¸'].unique().tolist()"
      ],
      "id": "7e3d9205",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b50b7b4"
      },
      "source": [
        "# <font color=red>__íŒŒìƒë³€ìˆ˜ğŸ˜€ğŸ˜€__</font>"
      ],
      "id": "2b50b7b4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10499c52"
      },
      "source": [
        "# ì¦ê°ìœ¨ê³„ì‚° í•¨ìˆ˜\n",
        "def rate_variation(f1,f2):\n",
        "    rate_variation = (f2-f1)/f1\n",
        "    rate_variation.fillna('ë¶€ì •',inplace=True) # Na : ì „ í›„ ë‘˜ë‹¤ 0ì¸ ê²½ìš°\n",
        "    rate_variation.replace({np.inf:'ë¶ˆëŠ¥'},inplace=True) # inf : ì „ë°˜ê¸° 0ì¸ ê²½ìš°\n",
        "    return rate_variation\n",
        "\n",
        "# ì¦ê°í­ê³„ì‚° í•¨ìˆ˜\n",
        "def variation(f1,f2):\n",
        "    variation = abs(f2-f1)\n",
        "    return variation"
      ],
      "id": "10499c52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7cfcd56"
      },
      "source": [
        "# êµ¬ë§¤íšŸìˆ˜ ì¦ê°\n",
        "def p_freq_ratio(n1,n2):\n",
        "    êµ¬ë§¤íšŸìˆ˜ = pd.pivot_table(ê¸°ì¡´ê³ ê°.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),\n",
        "             index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns='half',\n",
        "             values='ì˜ìˆ˜ì¦ë²ˆí˜¸',\n",
        "             aggfunc='count').fillna(0)\n",
        "    êµ¬ë§¤íšŸìˆ˜ì¦ê° = rate_variation(êµ¬ë§¤íšŸìˆ˜[n1],êµ¬ë§¤íšŸìˆ˜[n2])\n",
        "    return pd.Series(êµ¬ë§¤íšŸìˆ˜ì¦ê°,name=f'p_freq_ratio')"
      ],
      "id": "c7cfcd56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5e82a82"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°\n",
        "def cat1_p_amount_ratio(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='sum').fillna(0)\n",
        "    cat1_p_amount = rate_variation(ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n1],ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n2])\n",
        "    for colname in cat1_p_amount.columns:\n",
        "        cat1_p_amount.rename(columns={colname:f'cat1_{colname}_p_amount_ratio'},inplace=True)\n",
        "    return cat1_p_amount"
      ],
      "id": "f5e82a82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "767f4585"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ë¬¼í’ˆìˆ˜ì¦ê°\n",
        "def cat1_p_freq_ratio(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='count').fillna(0)\n",
        "    cat1_p_freq = rate_variation(ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n1],ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n2])\n",
        "    for colname in cat1_p_freq.columns:\n",
        "        cat1_p_freq.rename(columns={colname:f'cat1_{colname}_p_freq_ratio'},inplace=True)\n",
        "    return cat1_p_freq"
      ],
      "id": "767f4585",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5daed54"
      },
      "source": [
        "# ìµœëŒ€ êµ¬ë§¤íšŸìˆ˜ ì œíœ´ì‚¬(ë‘ ë°˜ê¸°)\n",
        "def max_affiliate_p_freq(n1,n2):\n",
        "    max_êµ¬ë§¤íšŸìˆ˜ = pd.pivot_table(ê¸°ì¡´ê³ ê°.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns=['half','ì œíœ´ì‚¬'],\n",
        "              values='ì˜ìˆ˜ì¦ë²ˆí˜¸',\n",
        "              aggfunc='count')\n",
        "    max1 = pd.Series(max_êµ¬ë§¤íšŸìˆ˜[n1].transpose().idxmax(),name=n1)\n",
        "    max2 = pd.Series(max_êµ¬ë§¤íšŸìˆ˜[n2].transpose().idxmax(),name=n2)\n",
        "    max_affiliate = pd.concat([max1,max2],axis=1)\n",
        "    max_affiliate['max_affiliate'] = max_affiliate.astype(str).apply('_'.join,axis=1)\n",
        "    return max_affiliate['max_affiliate']"
      ],
      "id": "d5daed54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62dcc7b4"
      },
      "source": [
        "# channel ì´ìš©ìœ ë¬´ : ì´ìš©ì€ 1, ì´ìš©ì•ˆí–ˆìœ¼ë©´ 0\n",
        "def to_binary(x):\n",
        "    if x>0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "def channel():\n",
        "    channel = pd.pivot_table(ldata['channel'].query(f'ê³ ê°ë²ˆí˜¸=={ê¸°ì¡´ê³ ê°idx}'),\n",
        "                               index='ê³ ê°ë²ˆí˜¸',\n",
        "                               values='ì´ìš©íšŸìˆ˜',\n",
        "                               aggfunc='sum')\n",
        "    channel1 = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  values='ì œíœ´ì‚¬',\n",
        "                  aggfunc='count').join(channel).fillna(0)[['ì´ìš©íšŸìˆ˜']]    \n",
        "    return pd.Series(channel1['ì´ìš©íšŸìˆ˜'].apply(lambda x:to_binary(x)),name='channel')"
      ],
      "id": "62dcc7b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63d68d02"
      },
      "source": [
        "# ë°˜ê¸°ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°  \n",
        "def half_inde(df,*tuple_args):\n",
        "    buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})\n",
        "    for i in tuple_args:        \n",
        "        buy_sum[f'half_inde'] = (buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[1])]-buy_sum[(\"êµ¬ë§¤ê¸ˆì•¡\",i[0])])/buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[0])]\n",
        "        colname = buy_sum.columns.tolist()[:-1]\n",
        "        colname.append(f'half_inde_ratio')\n",
        "        buy_sum.columns = colname    \n",
        "    buy_sum.drop([('êµ¬ë§¤ê¸ˆì•¡', 1),\n",
        "                  ('êµ¬ë§¤ê¸ˆì•¡', 2),\n",
        "                  ('êµ¬ë§¤ê¸ˆì•¡', 3),\n",
        "                  ('êµ¬ë§¤ê¸ˆì•¡', 4)],axis=1,inplace=True)           \n",
        "    return buy_sum"
      ],
      "id": "63d68d02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf4e7414"
      },
      "source": [
        "# ì œíœ´ì‚¬ë³„ ë°˜ê¸°ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°  \n",
        "def affiliate_half_inde(df,*tuple_args):\n",
        "    affiliate_buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns=['half','ì œíœ´ì‚¬'],\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})       \n",
        "    for i in tuple_args:\n",
        "        for j in ['A','B','C']:   \n",
        "            affiliate_buy_sum['ì œíœ´ì‚¬ êµ¬ë§¤ì¦ê°'] = (affiliate_buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[1],j)]-affiliate_buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[0],j)])/affiliate_buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[0],j)]\n",
        "            colname =affiliate_buy_sum.columns.tolist()[:-1]\n",
        "            colname.append(f'affiliate_half_inde_{j}_ratio')\n",
        "            affiliate_buy_sum.columns = colname\n",
        "    affiliate_buy_sum.fillna('ë¶€ì •',inplace=True) # Na : ì „ í›„ ë‘˜ë‹¤ 0ì¸ ê²½ìš°\n",
        "    affiliate_buy_sum.replace({np.inf:'ë¶ˆëŠ¥'},inplace=True) # inf : ì „ë°˜ê¸° 0ì¸ ê²½ìš°\n",
        "    affiliate_buy_sum.drop([('êµ¬ë§¤ê¸ˆì•¡',1, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',1, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',1, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'C'),],axis=1,inplace=True)\n",
        "    return affiliate_buy_sum"
      ],
      "id": "cf4e7414",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b29ed19"
      },
      "source": [
        "# ì œíœ´ì‚¬ë³„ êµ¬ë§¤íšŸìˆ˜ì¦ê° \n",
        "def buy_count(df,*tuple_args):\n",
        "    count=pd.pivot_table(df.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns=['half','ì œíœ´ì‚¬'],\n",
        "              values=['ì˜ìˆ˜ì¦ë²ˆí˜¸'],\n",
        "              aggfunc={'ì˜ìˆ˜ì¦ë²ˆí˜¸':'count'})    \n",
        "    for i in tuple_args:\n",
        "        for j in ['A','B','C']:   \n",
        "            count['ì œíœ´ì‚¬ êµ¬ë§¤íšŸìˆ˜ì¦ê°'] = (count[('ì˜ìˆ˜ì¦ë²ˆí˜¸',i[1],j)]-count[('ì˜ìˆ˜ì¦ë²ˆí˜¸',i[0],j)])/count[('ì˜ìˆ˜ì¦ë²ˆí˜¸',i[0],j)]\n",
        "            colname =count.columns.tolist()[:-1]\n",
        "            colname.append(f'buy_count_{j}_ratio')\n",
        "            count.columns = colname\n",
        "    count.fillna('ë¶€ì •',inplace=True) # Na : ì „ í›„ ë‘˜ë‹¤ 0ì¸ ê²½ìš°\n",
        "    count.replace({np.inf:'ë¶ˆëŠ¥'},inplace=True) # inf : ì „ë°˜ê¸° 0ì¸ ê²½ìš°\n",
        "    count.drop([('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"C\")],axis=1,inplace=True)  \n",
        "    return count"
      ],
      "id": "4b29ed19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc1c21d1"
      },
      "source": [
        "# ì¼íšŒí‰ê·  êµ¬ë§¤ê¸ˆì•¡ ì¦ê°\n",
        "def one_buy_mean_diff(df,*args_tuple):\n",
        "    buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})    \n",
        "    count=pd.pivot_table(df.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['ì˜ìˆ˜ì¦ë²ˆí˜¸'],\n",
        "              aggfunc={'ì˜ìˆ˜ì¦ë²ˆí˜¸':'count'})    \n",
        "    one_buy_mean_diff=buy_sum.join(count)   \n",
        "    for i in args_tuple:\n",
        "        cust_first_mean = one_buy_mean_diff[('êµ¬ë§¤ê¸ˆì•¡',i[0])]/one_buy_mean_diff[('ì˜ìˆ˜ì¦ë²ˆí˜¸', i[0])]\n",
        "        cust_second_mean = one_buy_mean_diff[('êµ¬ë§¤ê¸ˆì•¡',i[1])]/one_buy_mean_diff[('ì˜ìˆ˜ì¦ë²ˆí˜¸', i[1])]\n",
        "        one_buy_mean_diff['êµ¬ë§¤í‰ê· ì¦ê°'] = (cust_second_mean - cust_first_mean)/cust_first_mean\n",
        "        colname = one_buy_mean_diff.columns.tolist()[:-1]\n",
        "        colname.append(f'one_buy_mean_diff_ratio')\n",
        "        one_buy_mean_diff.columns = colname\n",
        "    one_buy_mean_diff.drop([('êµ¬ë§¤ê¸ˆì•¡', 1),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 2),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 3),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 4),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4)],axis=1,inplace=True)    \n",
        "    return one_buy_mean_diff"
      ],
      "id": "cc1c21d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a184debf"
      },
      "source": [
        "# êµ¬ë§¤íšŸìˆ˜ ì¦ê°í­\n",
        "def p_freq_wide(n1,n2):\n",
        "    êµ¬ë§¤íšŸìˆ˜ = pd.pivot_table(ê¸°ì¡´ê³ ê°.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),\n",
        "             index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns='half',\n",
        "             values='ì˜ìˆ˜ì¦ë²ˆí˜¸',\n",
        "             aggfunc='count').fillna(0)\n",
        "    êµ¬ë§¤íšŸìˆ˜ì¦ê° = abs(êµ¬ë§¤íšŸìˆ˜[n2]-êµ¬ë§¤íšŸìˆ˜[n1])\n",
        "    return pd.Series(êµ¬ë§¤íšŸìˆ˜ì¦ê°,name=f'p_freq_wide')"
      ],
      "id": "a184debf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "449d2a48"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°í­\n",
        "def cat1_p_amount_wide(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='sum').fillna(0)\n",
        "    cat1_p_amount = abs(ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n2]-ëŒ€ë¶„ë¥˜êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n1])\n",
        "    for colname in cat1_p_amount.columns:\n",
        "        cat1_p_amount.rename(columns={colname:f'cat1_{colname}_p_amount_wide'},inplace=True)\n",
        "    return cat1_p_amount"
      ],
      "id": "449d2a48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49f7d4ee"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ë¬¼í’ˆìˆ˜ì¦ê°\n",
        "def cat1_p_freq_wide(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='count').fillna(0)\n",
        "    cat1_p_freq = abs(ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n2]-ëŒ€ë¶„ë¥˜êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n1])\n",
        "    for colname in cat1_p_freq.columns:\n",
        "        cat1_p_freq.rename(columns={colname:f'cat1_{colname}_p_freq_wide'},inplace=True)\n",
        "    return cat1_p_freq"
      ],
      "id": "49f7d4ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a88cf829"
      },
      "source": [
        "# ë°˜ê¸°ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°í­\n",
        "def half_inde_wide(df,*tuple_args):\n",
        "    buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})\n",
        "    for i in tuple_args:        \n",
        "        buy_sum[f'half_inde{i[1]}_{i[0]}'] = abs(buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[1])]-buy_sum[(\"êµ¬ë§¤ê¸ˆì•¡\",i[0])])\n",
        "        colname = buy_sum.columns.tolist()[:-1]\n",
        "        colname.append('half_inde_wide')\n",
        "        buy_sum.columns = colname    \n",
        "    buy_sum.drop([('êµ¬ë§¤ê¸ˆì•¡', 1),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 2),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 3),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 4)],axis=1,inplace=True)        \n",
        "    return buy_sum"
      ],
      "id": "a88cf829",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c17b9c3"
      },
      "source": [
        "# ì œíœ´ì‚¬ë³„ ë°˜ê¸°ë³„ êµ¬ë§¤ê¸ˆì•¡ì¦ê°í­\n",
        "def affiliate_half_inde_wide(df,*tuple_args):\n",
        "    affiliate_buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns=['half','ì œíœ´ì‚¬'],\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})       \n",
        "    for i in tuple_args:\n",
        "        for j in ['A','B','C']:   \n",
        "            affiliate_buy_sum['ì œíœ´ì‚¬ êµ¬ë§¤ì¦ê°'] = abs(affiliate_buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[1],j)]-affiliate_buy_sum[('êµ¬ë§¤ê¸ˆì•¡',i[0],j)])\n",
        "            colname =affiliate_buy_sum.columns.tolist()[:-1]\n",
        "            colname.append(f'affiliate_{j}_half_inde_wide')\n",
        "            affiliate_buy_sum.columns = colname\n",
        "    affiliate_buy_sum.fillna(0,inplace=True)        \n",
        "    affiliate_buy_sum.drop([('êµ¬ë§¤ê¸ˆì•¡',1, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',1, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',1, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',2, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',3, 'C'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'A'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'B'),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡',4, 'C'),],axis=1,inplace=True)\n",
        "    return affiliate_buy_sum"
      ],
      "id": "2c17b9c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73370d63"
      },
      "source": [
        "# ì œíœ´ì‚¬ë³„ êµ¬ë§¤íšŸìˆ˜ì¦ê°í­\n",
        "def buy_count_wide(df,*tuple_args):\n",
        "    count=pd.pivot_table(df.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns=['half','ì œíœ´ì‚¬'],\n",
        "              values=['ì˜ìˆ˜ì¦ë²ˆí˜¸'],\n",
        "              aggfunc={'ì˜ìˆ˜ì¦ë²ˆí˜¸':'count'})    \n",
        "    for i in tuple_args:\n",
        "        for j in ['A','B','C']:   \n",
        "            count['ì œíœ´ì‚¬ êµ¬ë§¤íšŸìˆ˜ì¦ê°'] = abs(count[('ì˜ìˆ˜ì¦ë²ˆí˜¸',i[1],j)]-count[('ì˜ìˆ˜ì¦ë²ˆí˜¸',i[0],j)])\n",
        "            colname =count.columns.tolist()[:-1]\n",
        "            colname.append(f'buy_count_{j}_wide')\n",
        "            count.columns = colname\n",
        "    count.fillna(0,inplace=True)\n",
        "    count.drop([('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3,\"C\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"A\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"B\"),\n",
        "        ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4,\"C\")],axis=1,inplace=True)   \n",
        "    return count"
      ],
      "id": "73370d63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcc0358c"
      },
      "source": [
        "# ì¼íšŒí‰ê·  êµ¬ë§¤ê¸ˆì•¡ ì¦ê°í­\n",
        "def one_buy_mean_diff_wide(df,*args_tuple):\n",
        "    buy_sum=pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['êµ¬ë§¤ê¸ˆì•¡'],\n",
        "              aggfunc={'êµ¬ë§¤ê¸ˆì•¡':'sum'})    \n",
        "    count=pd.pivot_table(df.drop_duplicates('ì˜ìˆ˜ì¦ë²ˆí˜¸'),index='ê³ ê°ë²ˆí˜¸',\n",
        "              columns='half',\n",
        "              values=['ì˜ìˆ˜ì¦ë²ˆí˜¸'],\n",
        "              aggfunc={'ì˜ìˆ˜ì¦ë²ˆí˜¸':'count'})    \n",
        "    one_buy_mean_diff=buy_sum.join(count)   \n",
        "    for i in args_tuple:\n",
        "        cust_first_mean = one_buy_mean_diff[('êµ¬ë§¤ê¸ˆì•¡',i[0])]/one_buy_mean_diff[('ì˜ìˆ˜ì¦ë²ˆí˜¸', i[0])]\n",
        "        cust_second_mean = one_buy_mean_diff[('êµ¬ë§¤ê¸ˆì•¡',i[1])]/one_buy_mean_diff[('ì˜ìˆ˜ì¦ë²ˆí˜¸', i[1])]\n",
        "        one_buy_mean_diff['êµ¬ë§¤í‰ê· ì¦ê°'] = abs(cust_second_mean - cust_first_mean)\n",
        "        colname = one_buy_mean_diff.columns.tolist()[:-1]\n",
        "        colname.append(f'one_buy_mean_diff_wide')\n",
        "        one_buy_mean_diff.columns = colname\n",
        "    one_buy_mean_diff.drop([('êµ¬ë§¤ê¸ˆì•¡', 1),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 2),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 3),\n",
        "            ('êµ¬ë§¤ê¸ˆì•¡', 4),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 1),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 2),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 3),\n",
        "            ('ì˜ìˆ˜ì¦ë²ˆí˜¸', 4)],axis=1,inplace=True)    \n",
        "    return one_buy_mean_diff"
      ],
      "id": "dcc0358c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590507ac"
      },
      "source": [
        "# ê°€ê²©ëŒ€ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n",
        "def price_ratio(n1,n2):\n",
        "    def ì €ì¤‘ê³ (x):\n",
        "        if x[-2:] == 'ì €ê°€':\n",
        "            return 'ì €ê°€'\n",
        "        elif x[-2:] == 'ì¤‘ê°€':\n",
        "            return 'ì¤‘ê°€'\n",
        "        elif x[-2:] == 'ê³ ê°€':\n",
        "            return 'ê³ ê°€'\n",
        "    ê¸°ì¡´ê³ ê°['ê°€ê²©ëŒ€'] = ê¸°ì¡´ê³ ê°['ìƒí’ˆë¶„ë¥˜'].apply(lambda x: ì €ì¤‘ê³ (x))\n",
        "    ê°€ê²©ëŒ€ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                        index='ê³ ê°ë²ˆí˜¸',\n",
        "                        columns=['half','ê°€ê²©ëŒ€'],\n",
        "                        values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                        aggfunc='sum')\n",
        "    ê°€ê²©ëŒ€.fillna(0,inplace=True)\n",
        "    ê°€ê²©ëŒ€ì¦ê°ìœ¨ = rate_variation(ê°€ê²©ëŒ€[n1],ê°€ê²©ëŒ€[n2])    \n",
        "    for colname in ê°€ê²©ëŒ€ì¦ê°ìœ¨.columns:\n",
        "        ê°€ê²©ëŒ€ì¦ê°ìœ¨.rename(columns={colname:f'price_{colname}_ratio'},inplace=True)       \n",
        "    return ê°€ê²©ëŒ€ì¦ê°ìœ¨"
      ],
      "id": "590507ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "284fd784"
      },
      "source": [
        "# ê°€ê²©ëŒ€ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°í­\n",
        "def price_wide(n1,n2):\n",
        "    def ì €ì¤‘ê³ (x):\n",
        "        if x[-2:] == 'ì €ê°€':\n",
        "            return 'ì €ê°€'\n",
        "        elif x[-2:] == 'ì¤‘ê°€':\n",
        "            return 'ì¤‘ê°€'\n",
        "        elif x[-2:] == 'ê³ ê°€':\n",
        "            return 'ê³ ê°€'\n",
        "    ê¸°ì¡´ê³ ê°['ê°€ê²©ëŒ€'] = ê¸°ì¡´ê³ ê°['ìƒí’ˆë¶„ë¥˜'].apply(lambda x: ì €ì¤‘ê³ (x))\n",
        "    ê°€ê²©ëŒ€ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                        index='ê³ ê°ë²ˆí˜¸',\n",
        "                        columns=['half','ê°€ê²©ëŒ€'],\n",
        "                        values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                        aggfunc='sum')\n",
        "    ê°€ê²©ëŒ€.fillna(0,inplace=True)\n",
        "    ê°€ê²©ëŒ€ì¦ê°í­ = variation(ê°€ê²©ëŒ€[n1],ê°€ê²©ëŒ€[n2])        \n",
        "    for colname in ê°€ê²©ëŒ€ì¦ê°í­.columns:\n",
        "        ê°€ê²©ëŒ€ì¦ê°í­.rename(columns={colname:f'price_{colname}_wide'},inplace=True)       \n",
        "    return ê°€ê²©ëŒ€ì¦ê°í­"
      ],
      "id": "284fd784",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29674c69"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜/ê°€ê²©ëŒ€ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n",
        "def cat1_price_rate(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                        index='ê³ ê°ë²ˆí˜¸',\n",
        "                        columns=['half','ìƒí’ˆë¶„ë¥˜'],\n",
        "                        values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                        aggfunc='sum')\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€.fillna(0,inplace=True)\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°ìœ¨ = rate_variation(ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€[n1],ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€[n2])\n",
        "    for colname in ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°ìœ¨.columns:\n",
        "        ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°ìœ¨.rename(columns={colname:f'cat1_{colname}_price_ratio'},inplace=True)       \n",
        "    return ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°ìœ¨"
      ],
      "id": "29674c69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80756064"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜/ê°€ê²©ëŒ€ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°í­\n",
        "def cat1_price_wide(n1,n2):\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                        index='ê³ ê°ë²ˆí˜¸',\n",
        "                        columns=['half','ìƒí’ˆë¶„ë¥˜'],\n",
        "                        values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                        aggfunc='sum')\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€.fillna(0,inplace=True)\n",
        "    ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°í­ = variation(ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€[n1],ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€[n2])\n",
        "    for colname in ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°í­.columns:\n",
        "        ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°í­.rename(columns={colname:f'cat1_{colname}_price_wide'},inplace=True)       \n",
        "    return ëŒ€ë¶„ë¥˜ê°€ê²©ëŒ€ì¦ê°í­.fillna(0)"
      ],
      "id": "80756064",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b92fdcd7"
      },
      "source": [
        "# ëŒ€ë¶„ë¥˜ë³„ ê¸°ì—¬ìœ¨\n",
        "def cont_ratio_cat1(n1,n2):\n",
        "    cat1 = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "                  values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                  aggfunc='sum')\n",
        "    cat1_1 = cat1[n1].fillna(0)\n",
        "    cat1_1['sum'] = cat1_1.sum(axis=1)\n",
        "    cat1_2 = cat1[n2].fillna(0)\n",
        "    cat1_2['sum'] = cat1_2.sum(axis=1)\n",
        "    cat1ê¸°ì—¬ìœ¨ = cat1_2-cat1_1\n",
        "    for i in ê¸°ì¡´ê³ ê°['ëŒ€ë¶„ë¥˜ëª…'].unique():\n",
        "        cat1ê¸°ì—¬ìœ¨[i] = cat1ê¸°ì—¬ìœ¨[i].div(cat1ê¸°ì—¬ìœ¨['sum'])*100\n",
        "    for colname in cat1ê¸°ì—¬ìœ¨.columns:\n",
        "        cat1ê¸°ì—¬ìœ¨.rename(columns={colname:f'cat1_{colname}_cont_ratio'},inplace=True)\n",
        "    return cat1ê¸°ì—¬ìœ¨.drop('cat1_sum_cont_ratio',axis=1)"
      ],
      "id": "b92fdcd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46cfd9e8"
      },
      "source": [
        "# ì œíœ´ì‚¬ë³„ ê¸°ì—¬ìœ¨\n",
        "def cont_ratio_affiliate(n1,n2):\n",
        "    ì œíœ´ì‚¬ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  columns=['half','ì œíœ´ì‚¬'],\n",
        "                  values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                  aggfunc='sum')\n",
        "    ì œíœ´ì‚¬1 = ì œíœ´ì‚¬[n1].fillna(0)\n",
        "    ì œíœ´ì‚¬1['sum'] = ì œíœ´ì‚¬1.sum(axis=1)\n",
        "    ì œíœ´ì‚¬2 = ì œíœ´ì‚¬[n2].fillna(0)\n",
        "    ì œíœ´ì‚¬2['sum'] = ì œíœ´ì‚¬2.sum(axis=1)\n",
        "    ì œíœ´ì‚¬ê¸°ì—¬ìœ¨ = ì œíœ´ì‚¬2-ì œíœ´ì‚¬1\n",
        "    for i in ['A','B','C']:\n",
        "        ì œíœ´ì‚¬ê¸°ì—¬ìœ¨[i] = ì œíœ´ì‚¬ê¸°ì—¬ìœ¨[i].div(ì œíœ´ì‚¬ê¸°ì—¬ìœ¨['sum'])*100\n",
        "    for colname in ì œíœ´ì‚¬ê¸°ì—¬ìœ¨.columns:\n",
        "        ì œíœ´ì‚¬ê¸°ì—¬ìœ¨.rename(columns={colname:f'affiliate_{colname}_cont_ratio'},inplace=True)\n",
        "    return ì œíœ´ì‚¬ê¸°ì—¬ìœ¨.drop('affiliate_sum_cont_ratio',axis=1).fillna(0)"
      ],
      "id": "46cfd9e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93f631f1"
      },
      "source": [
        "def class1_p_amount(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='sum').fillna(0)\n",
        "    class1_p_amount = rate_variation(ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n1],ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n2])\n",
        "    for colname in class1_p_amount.columns:\n",
        "        class1_p_amount.rename(columns={colname:f'class1_{colname}_p_amount_ratio'},inplace=True)\n",
        "    return class1_p_amount"
      ],
      "id": "93f631f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a65f586"
      },
      "source": [
        "def class1_p_freq(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='count').fillna(0)\n",
        "    class1_p_freq = rate_variation(ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n1],ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n2])\n",
        "    for colname in class1_p_freq.columns:\n",
        "        class1_p_freq.rename(columns={colname:f'class1_{colname}_p_freq_ratio'},inplace=True)\n",
        "    return class1_p_freq"
      ],
      "id": "5a65f586",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f810bc6"
      },
      "source": [
        "def class1_p_amount_wide(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='sum').fillna(0)\n",
        "    class1_p_amount = abs(ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n2]-ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n1])\n",
        "    for colname in class1_p_amount.columns:\n",
        "        class1_p_amount.rename(columns={colname:f'class1_{colname}_p_amount_wide'},inplace=True)\n",
        "    return class1_p_amount"
      ],
      "id": "6f810bc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183e7028"
      },
      "source": [
        "def class1_p_freq_wide(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='count').fillna(0)\n",
        "    class1_p_freq = abs(ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n2]-ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n1])\n",
        "    for colname in class1_p_freq.columns:\n",
        "        class1_p_freq.rename(columns={colname:f'class1_{colname}_p_freq_wide'},inplace=True)\n",
        "    return class1_p_freq"
      ],
      "id": "183e7028",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4542af1e"
      },
      "source": [
        "def class1_p_freq_net(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='count').fillna(0)\n",
        "    class1_p_freq = ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n2]-ë¶„ë¥˜1êµ¬ë§¤ë¬¼í’ˆìˆ˜í•©ê³„[n1]\n",
        "    for colname in class1_p_freq.columns:\n",
        "        class1_p_freq.rename(columns={colname:f'class1_{colname}_p_freq_net'},inplace=True)\n",
        "    return class1_p_freq"
      ],
      "id": "4542af1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35eb6591"
      },
      "source": [
        "def class1_p_amount_net(n1,n2):\n",
        "    ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„ = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "              index='ê³ ê°ë²ˆí˜¸',\n",
        "             columns=['half','ë¶„ë¥˜1'],\n",
        "              values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "              aggfunc='sum').fillna(0)\n",
        "    class1_p_amount = ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n2]-ë¶„ë¥˜1êµ¬ë§¤ê¸ˆì•¡í•©ê³„[n1]\n",
        "    for colname in class1_p_amount.columns:\n",
        "        class1_p_amount.rename(columns={colname:f'class1_{colname}_p_amount_net'},inplace=True)\n",
        "    return class1_p_amount"
      ],
      "id": "35eb6591",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "481b4785"
      },
      "source": [
        "# ì‹í’ˆë¹„ì¤‘\n",
        "def food_w(n1,n2):\n",
        "    p_amount = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  columns=['half','ëŒ€ë¶„ë¥˜ëª…'],\n",
        "                  values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                  aggfunc='sum')\n",
        "    p_amount_t = p_amount[n1] + p_amount[n2]\n",
        "    p_amount_t['sum'] = p_amount_t.sum(axis=1)\n",
        "    p_amount_t['food_w'] = p_amount_t['ê°€ê³µì‹í’ˆ'] + p_amount_t['ì‹ ì„ ì‹í’ˆ']\n",
        "    food_w = p_amount_t['food_w'].div(p_amount_t['sum'],axis=0)*100\n",
        "    return food_w.fillna(0)"
      ],
      "id": "481b4785",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c96fb68"
      },
      "source": [
        "# ì„ ë§¤í’ˆ/ì¼ìƒí’ˆ/ì „ë¬¸í’ˆ ë¹„ì¤‘\n",
        "def class1_w(n1,n2):\n",
        "    class1_amount = pd.pivot_table(ê¸°ì¡´ê³ ê°,\n",
        "                  index='ê³ ê°ë²ˆí˜¸',\n",
        "                  columns=['half','ë¶„ë¥˜1'],\n",
        "                  values='êµ¬ë§¤ê¸ˆì•¡',\n",
        "                  aggfunc='sum')\n",
        "    class1_amount_t = class1_amount[n1] + class1_amount[n2]\n",
        "    class1_amount_t['sum'] = class1_amount_t.sum(axis=1)\n",
        "    class1_w = class1_amount_t.drop('sum',axis=1).div(class1_amount_t['sum'],axis=0)*100\n",
        "    for colname in class1_w.columns:\n",
        "        class1_w.rename(columns={colname:f'class1_w_{colname}'},inplace=True)\n",
        "    return  class1_w.fillna(0)"
      ],
      "id": "0c96fb68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2cda74d"
      },
      "source": [
        "# ì¢…ì†ë³€ìˆ˜\n",
        "def label(df,half1,half2):\n",
        "    sale_different = pd.pivot_table(df,index='ê³ ê°ë²ˆí˜¸',\n",
        "                                   columns = 'half',\n",
        "                                   values = 'êµ¬ë§¤ê¸ˆì•¡',\n",
        "                                   aggfunc= 'sum')\n",
        "    ì „ì²´ì¦ê°ìœ¨ = (sum(sale_different[int(f'{half2}')])-sum(sale_different[int(f'{half1}')]))/sum(sale_different[int(f'{half1}')])\n",
        "    sale_different['label'] = (sale_different[int(f'{half2}')] - sale_different[int(f'{half1}')])/sale_different[int(f'{half1}')]/ì „ì²´ì¦ê°ìœ¨\n",
        "    def to_label(ì¦ê°):\n",
        "        if ì¦ê°>=1:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "    sale_different[f'label'] = sale_different[f'label'].apply(lambda x: to_label(x))\n",
        "    sale_different = sale_different[[f'label']]\n",
        "    sale_different.columns = [f'label'] \n",
        "    return sale_different"
      ],
      "id": "d2cda74d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2f02d9"
      },
      "source": [
        " # <font color=red>__ë°ì´í„°ì…‹ğŸ˜€ğŸ˜€__</font>"
      ],
      "id": "9e2f02d9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80758190"
      },
      "source": [
        "def make_dataset(df,n1,n2,n3):\n",
        "    dataset = pd.DataFrame(p_freq_ratio(n1,n2)).join([cat1_p_amount_ratio(n1,n2), # ë°˜ê¸°ë³„êµ¬ë§¤íšŸìˆ˜ì¦ê°ìœ¨, ëŒ€ë¶„ë¥˜ë³„êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                   cat1_p_freq_ratio(n1,n2), # ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ë¬¼í’ˆìˆ˜ì¦ê°ìœ¨\n",
        "                                   max_affiliate_p_freq(n1,n2), # ìµœëŒ€êµ¬ë§¤ê¸ˆì•¡ì œíœ´ì‚¬ ë³€ë™\n",
        "                                   channel(), # ì±„ë„ì´ìš©ìœ ë¬´\n",
        "                                   half_inde(df,(n1,n2)), # ë°˜ê¸°ë³„êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                   affiliate_half_inde(df,(n1,n2)), # ì œíœ´ì‚¬ë³„êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                   buy_count(df,(n1,n2)), # ë°˜ê¸°ë³„êµ¬ë§¤íšŸìˆ˜ì¦ê°ìœ¨\n",
        "                                   one_buy_mean_diff(df,(n1,n2)), # 1íšŒí‰ê· êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                                \n",
        "                                   ldata['cust'].set_index('ê³ ê°ë²ˆí˜¸'), # ì„±ë³„, ì—°ë ¹ëŒ€, ê±°ì£¼ì§€ì—­\n",
        "                                                \n",
        "                                   p_freq_wide(n1,n2), # êµ¬ë§¤íšŸìˆ˜ë³€ë™í­\n",
        "                                   cat1_p_amount_wide(n1,n2), # ëŒ€ë¶„ë¥˜ë³„êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                   cat1_p_freq_wide(n1,n2), # ëŒ€ë¶„ë¥˜ë³„êµ¬ë§¤ë¬¼í’ˆìˆ˜ë³€ë™í­\n",
        "                                   half_inde_wide(df,(n1,n2)), # ë°˜ê¸°ë³„êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                   affiliate_half_inde_wide(df,(n1,n2)), # ì œíœ´ì‚¬ë³„êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                   buy_count_wide(df,(n1,n2)), # ë°˜ê¸°ë³„êµ¬ë§¤íšŸìˆ˜ë³€ë™í­\n",
        "                                   one_buy_mean_diff_wide(df,(n1,n2)), # 1íšŒí‰ê· êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                                \n",
        "                                   price_ratio(n1,n2), # ê°€ê²©ëŒ€êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                   price_wide(n1,n2), # ê°€ê²©ëŒ€êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                   cat1_price_rate(n1,n2), # ëŒ€ë¶„ë¥˜ë³„ê°€ê²©ëŒ€êµ¬ë§¤ê¸ˆì•¡ì¦ê°ìœ¨\n",
        "                                   cat1_price_wide(n1,n2), # ëŒ€ë¶„ë¥˜ë³„ê°€ê²©ëŒ€êµ¬ë§¤ê¸ˆì•¡ë³€ë™í­\n",
        "                                   cont_ratio_cat1(n1,n2), # ëŒ€ë¶„ë¥˜ë³„êµ¬ë§¤ê¸ˆì•¡ê¸°ì—¬ìœ¨\n",
        "                                   cont_ratio_affiliate(n1,n2), # ì œíœ´ì‚¬ë³„êµ¬ë§¤ê¸ˆì•¡ê¸°ì—¬ìœ¨\n",
        "                                                \n",
        "                                   class1_p_amount(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n",
        "                                   class1_p_freq(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤íšŸìˆ˜ ì¦ê°ìœ¨\n",
        "                                   class1_p_amount_wide(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤ê¸ˆì•¡ ë³€ë™í­\n",
        "                                   class1_p_freq_wide(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤íšŸìˆ˜ ë³€ë™í­\n",
        "                                   class1_p_freq_net(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤íšŸìˆ˜ ì¦ê°\n",
        "                                   class1_p_amount_net(n1,n2), # ë¶„ë¥˜1 êµ¬ë§¤ê¸ˆì•¡ ì¦ê°\n",
        "                                   label(df,n1,n3)]) # ì¢…ì†ë³€ìˆ˜\n",
        "    return dataset\n",
        "\n",
        "dataset1_2 = make_dataset(ê¸°ì¡´ê³ ê°,1,2,3)\n",
        "dataset2_3 = make_dataset(ê¸°ì¡´ê³ ê°,2,3,4)"
      ],
      "id": "80758190",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "948a3887"
      },
      "source": [
        "#### ì¹´í…Œê³ ë¦¬ ì „\n",
        "dataset1_2.to_excel('dataset_1017/dataset1_2.xlsx')\n",
        "dataset2_3.to_excel('dataset_1017/dataset2_3.xlsx')"
      ],
      "id": "948a3887",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6686863a"
      },
      "source": [
        "## ë²”ì£¼í™”"
      ],
      "id": "6686863a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d364e3"
      },
      "source": [
        "dataset1_2_cat = dataset1_2.copy()\n",
        "dataset2_3_cat = dataset2_3.copy()"
      ],
      "id": "43d364e3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e58bc33"
      },
      "source": [
        "dataset1_2_cat.drop(['cat1_ê¸°íƒ€ì €ê°€_price_ratio','cat1_ê¸°íƒ€ì €ê°€_price_wide','cat1_ì˜ì•½ê³ ê°€_price_wide','cat1_ì˜ì•½ì €ê°€_price_ratio','cat1_ì˜ì•½ì¤‘ê°€_price_ratio','cat1_ì˜ì•½ê³ ê°€_price_ratio'],axis=1,inplace=True)\n",
        "dataset2_3_cat.drop(['cat1_ê¸°íƒ€ì €ê°€_price_ratio','cat1_ê¸°íƒ€ì €ê°€_price_wide','cat1_ì˜ì•½ê³ ê°€_price_wide','cat1_ì˜ì•½ì €ê°€_price_ratio','cat1_ì˜ì•½ì¤‘ê°€_price_ratio','cat1_ì˜ì•½ê³ ê°€_price_ratio'],axis=1,inplace=True)"
      ],
      "id": "1e58bc33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "356e8d8c"
      },
      "source": [
        "# ì¹´í…Œê³ ë¦¬ ëŒ€ìƒ ì»¬ëŸ¼  \n",
        "\n",
        "to_cat_columns =list(dataset1_2_cat.columns[dataset1_2_cat.columns.str.contains('ratio')])\n",
        "to_cat_wide_columns = list(dataset1_2_cat.columns[dataset1_2_cat.columns.str.contains('wide')])\n",
        "to_cat_net_columns = list(dataset1_2_cat.columns[dataset1_2_cat.columns.str.contains('net')])\n",
        "le_columns = ['ì„±ë³„','ì—°ë ¹ëŒ€','ê±°ì£¼ì§€ì—­','max_affiliate']"
      ],
      "id": "356e8d8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbef1674"
      },
      "source": [
        "def to_cat(df, col,n=4):\n",
        "    data = df[col]\n",
        "    data_float = data[(data!='ë¶ˆëŠ¥')&(data!='ë¶€ì •')].astype(float)\n",
        "    if len(data_float) != 0:\n",
        "        data_minus = data_float[data_float<=0]\n",
        "        data_minus_qcut = pd.cut(data_minus,n,labels=list(range(1,n+1)))\n",
        "\n",
        "        data_plus = data_float[(0<data_float)&(data_float<=1)]\n",
        "        data_plus_qcut = pd.cut(data_plus,n,labels=list(range(1+n,n*2+1)))\n",
        "\n",
        "        data_float[1<data_float] = 2*n+1\n",
        "        data_outlier = data_float[1<data_float]\n",
        "\n",
        "        data_object = data[(data=='ë¶ˆëŠ¥')|(data=='ë¶€ì •')]\n",
        "        data_object.replace({'ë¶€ì •':0,'ë¶ˆëŠ¥':2*n+2},inplace=True)\n",
        "        return pd.concat([data_minus_qcut,data_plus_qcut,data_outlier,data_object])\n",
        "    \n",
        "    else:\n",
        "        data_object = data[(data=='ë¶ˆëŠ¥')|(data=='ë¶€ì •')]\n",
        "        data_object.replace({'ë¶€ì •':0,'ë¶ˆëŠ¥':1},inplace=True)\n",
        "        return data_object\n",
        "\n",
        "for i in to_cat_columns:\n",
        "    dataset1_2_cat[i] = to_cat(dataset1_2_cat,i)\n",
        "for i in to_cat_columns:\n",
        "    dataset2_3_cat[i] = to_cat(dataset2_3_cat,i)"
      ],
      "id": "bbef1674",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8571622b"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "for i in le_columns:\n",
        "    dataset1_2_cat[i] = le.fit_transform(dataset1_2_cat[i])\n",
        "for i in le_columns:\n",
        "    dataset2_3_cat[i] = le.fit_transform(dataset2_3_cat[i])"
      ],
      "id": "8571622b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa6ac758"
      },
      "source": [
        "def to_cat_wide(df,col,n=4):\n",
        "    data = df[col]\n",
        "    q1, q3 = np.percentile(df[col],[25,75])    \n",
        "    iqr=q3 - q1    \n",
        "    lower_bound = q1-(iqr * 1.5)\n",
        "    upper_bound = q3+(iqr * 1.5)\n",
        "\n",
        "    def deny(x):\n",
        "        if x >upper_bound:\n",
        "            return \"up\"\n",
        "        elif x < lower_bound:\n",
        "            return \"lo\"\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    data = df[col].apply(deny)\n",
        "    data_float = data[(data!='lo')&(data!='up')].astype(float)\n",
        "\n",
        "    if len(data_float) != 0:\n",
        "        data_float_cut = pd.cut(data_float,n,labels=list(range(1,n+1)))\n",
        "\n",
        "        data_object = data[(data=='lo')|(data=='up')]\n",
        "        data_object.replace({'up':n+1,'lo':0},inplace=True)\n",
        "\n",
        "        return pd.concat([data_float_cut,data_object])\n",
        "        \n",
        "    else:\n",
        "        return df[col].fillna(0)\n",
        "\n",
        "for i in to_cat_wide_columns:\n",
        "    dataset1_2_cat[i] = to_cat_wide(dataset1_2_cat,i)\n",
        "for i in to_cat_wide_columns:\n",
        "    dataset2_3_cat[i] = to_cat_wide(dataset2_3_cat,i)"
      ],
      "id": "aa6ac758",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2202bb43"
      },
      "source": [
        "def to_cat_net(df,col,n=4):\n",
        "    data = df[col]\n",
        "    q1, q3 = np.percentile(df[col],[25,75])    \n",
        "    iqr=q3 - q1    \n",
        "    lower_bound = q1-(iqr * 1.5)\n",
        "    upper_bound = q3+(iqr * 1.5)\n",
        "\n",
        "    def deny(x):\n",
        "        if x >upper_bound:\n",
        "            return \"up\"\n",
        "        elif x < lower_bound:\n",
        "            return \"lo\"\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    data = df[col].apply(deny)\n",
        "    data_float = data[(data!='lo')&(data!='up')].astype(float)\n",
        "\n",
        "    if len(data_float) != 0:\n",
        "        data_float_cut = pd.cut(data_float,n+1,labels=list(range(1,n+2)))\n",
        "\n",
        "        data_object = data[(data=='lo')|(data=='up')]\n",
        "        data_object.replace({'up':n+2,'lo':0},inplace=True)\n",
        "\n",
        "        return pd.concat([data_float_cut,data_object])\n",
        "        \n",
        "    else:\n",
        "        return df[col].fillna(0)\n",
        "    \n",
        "for i in to_cat_net_columns:\n",
        "    dataset1_2_cat[i] = to_cat_net(dataset1_2_cat,i)\n",
        "for i in to_cat_net_columns:\n",
        "    dataset2_3_cat[i] = to_cat_net(dataset2_3_cat,i)"
      ],
      "id": "2202bb43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38994422"
      },
      "source": [
        "#### ì¹´í…Œê³ ë¦¬ í›„\n",
        "dataset1_2_cat.to_excel('dataset_1017/dataset1_2_cat.xlsx')\n",
        "dataset2_3_cat.to_excel('dataset_1017/dataset2_3_cat.xlsx')"
      ],
      "id": "38994422",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-0gigb_5Ft3"
      },
      "source": [
        "### ëª¨ë¸ë§"
      ],
      "id": "h-0gigb_5Ft3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHd3deYS44Dr"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.svm as svm\n",
        "import sklearn.metrics as mt\n",
        "from xgboost import plot_importance\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, plot_importance"
      ],
      "id": "LHd3deYS44Dr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMZWAVQK5bJJ"
      },
      "source": [
        "# 3ê¸° ì˜ˆì¸¡\n",
        "\n",
        "X = dataset1_2.astype(float).drop('label',axis=1)\n",
        "y = dataset1_2['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 4)\n",
        "\n",
        "def metric(y_test, pred):\n",
        "    print('-----------------------------')\n",
        "    print('ì •í™•ë„ : ',round(mt.accuracy_score(y_test, pred),2))\n",
        "    print('ì •ë°€ë„ : ',round(mt.precision_score(y_test, pred),2))\n",
        "    print('ì¬í˜„ìœ¨ : ',round(mt.recall_score(y_test, pred),2))\n",
        "    print('f1 : ',round(mt.f1_score(y_test, pred),2))\n",
        "    print('\\n')\n",
        "    \n",
        "dct = DecisionTreeClassifier(criterion = 'entropy',max_depth = 10,random_state=2)\n",
        "rfc = RandomForestClassifier(n_estimators = 10)\n",
        "log = LogisticRegression()\n",
        "\n",
        "rfc.fit(X_train, y_train)\n",
        "dct.fit(X_train, y_train)\n",
        "log.fit(X_train, y_train)\n",
        "\n",
        "dct_pred = dct.predict(X_test)\n",
        "rfc_pred = rfc.predict(X_test)\n",
        "log_pred = log.predict(X_test)\n",
        "\n",
        "xgb_clf = XGBClassifier(n_estimators=300, random_state=156, learning_rate=0.1, max_depth=5) \n",
        "xgb_clf.fit(X_train, y_train,\n",
        "           eval_metric='auc', eval_set=[(X_train,y_train),(X_test,y_test)],verbose=False)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "\n",
        "lgb = LGBMClassifier()\n",
        "lgb.fit(X_train, y_train)\n",
        "lgb_pred = lgb.predict(X_test)\n",
        "\n",
        "print('ë¡œì§€ìŠ¤í‹±')\n",
        "metric(y_test, log_pred)\n",
        "print('ëœë¤í¬ë ˆìŠ¤íŠ¸')\n",
        "metric(y_test, rfc_pred)\n",
        "print('ë””ì‹œì ¼')\n",
        "metric(y_test, dct_pred)\n",
        "print('XGB')\n",
        "metric(y_test, xgb_pred)\n",
        "print('LightGBM')\n",
        "metric(y_test, lgb_pred)"
      ],
      "id": "WMZWAVQK5bJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zMjJ-l95i7C"
      },
      "source": [
        "# 1~3ê¸° ì˜ˆì¸¡ ëª¨ë¸ ì ìš©í•´ì„œ 4ê¸° ì˜ˆì¸¡\n",
        "\n",
        "X_test1 = dataset2_3.astype(float).drop('label',axis=1)\n",
        "y_test1 = dataset2_3['label']\n",
        "\n",
        "dct_pred1 = dct.predict(X_test1)\n",
        "rfc_pred1 = rfc.predict(X_test1)\n",
        "log_pred1 = log.predict(X_test1)\n",
        "xgb_pred1 = xgb_clf.predict(X_test1)\n",
        "lgb_pred1 = lgb.predict(X_test1)\n",
        "\n",
        "print('ë¡œì§€ìŠ¤í‹±')\n",
        "metric(y_test1, log_pred1)\n",
        "print('ëœë¤í¬ë ˆìŠ¤íŠ¸')\n",
        "metric(y_test1, rfc_pred1)\n",
        "print('ë””ì‹œì ¼')\n",
        "metric(y_test1, dct_pred1)\n",
        "print('XGB')\n",
        "metric(y_test1, xgb_pred1)\n",
        "print('LightGBM')\n",
        "metric(y_test1, lgb_pred1)"
      ],
      "id": "5zMjJ-l95i7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg2wHUdZ5w6C"
      },
      "source": [
        "# ë¡œì§€ìŠ¤í‹± íšŒê·€ê³„ìˆ˜ë¡œ í”¼ì²˜ ì„ íƒ í›„ í•™ìŠµ ë° ì˜ˆì¸¡\n",
        "import statsmodels.api as sm\n",
        "summary=sm.Logit(y, X).fit().summary().tables[1].as_html()\n",
        "pvalues = pd.read_html(summary, header=0, index_col=0)[0]\n",
        "display(pvalues,pvalues.iloc[:,3][pvalues.iloc[:,3]<0.05])\n",
        "\n",
        "columns = pvalues.iloc[:,3][pvalues.iloc[:,3]<0.05].index.tolist()\n",
        "\n",
        "# X = dataset_last1_2_cat1.drop('label',axis=1)\n",
        "# y = dataset_last1_2_cat1['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 4)\n",
        "\n",
        "def metric(y_test, pred):\n",
        "    print('-----------------------------')\n",
        "    print('ì •í™•ë„ : ',round(mt.accuracy_score(y_test, pred),2))\n",
        "    print('ì •ë°€ë„ : ',round(mt.precision_score(y_test, pred),2))\n",
        "    print('ì¬í˜„ìœ¨ : ',round(mt.recall_score(y_test, pred),2))\n",
        "    print('f1 : ',round(mt.f1_score(y_test, pred),2))\n",
        "    print('\\n')\n",
        "    \n",
        "dct = DecisionTreeClassifier(criterion = 'entropy',max_depth = 10,random_state=2)\n",
        "rfc = RandomForestClassifier(n_estimators = 10)\n",
        "log = LogisticRegression()\n",
        "\n",
        "rfc.fit(X_train, y_train)\n",
        "dct.fit(X_train, y_train)\n",
        "log.fit(X_train, y_train)\n",
        "\n",
        "dct_pred = dct.predict(X_test)\n",
        "rfc_pred = rfc.predict(X_test)\n",
        "log_pred = log.predict(X_test)\n",
        "\n",
        "xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.01, max_depth=5) \n",
        "xgb_clf.fit(X_train, y_train, early_stopping_rounds=100,\n",
        "           eval_metric='auc', eval_set=[(X_train,y_train),(X_test,y_test)],verbose=False)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "lgb = LGBMClassifier()\n",
        "lgb.fit(X_train, y_train)\n",
        "lgb_pred = lgb.predict(X_test)\n",
        "\n",
        "print('ë¡œì§€ìŠ¤í‹±')\n",
        "metric(y_test, log_pred)\n",
        "print('ëœë¤í¬ë ˆìŠ¤íŠ¸')\n",
        "metric(y_test, rfc_pred)\n",
        "print('ë””ì‹œì ¼')\n",
        "metric(y_test, dct_pred)\n",
        "print('XGB')\n",
        "metric(y_test, xgb_pred)\n",
        "print('LightGBM')\n",
        "metric(y_test, lgb_pred)"
      ],
      "id": "Kg2wHUdZ5w6C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1K4IpE36EOo"
      },
      "source": [
        "# feature importance í™•ì¸\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(random_state=4)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "ft_importance_values = model.feature_importances_\n",
        "\n",
        "ft_series = pd.Series(ft_importance_values, index = X_train.columns)\n",
        "ft_top30 = ft_series.sort_values(ascending=False)[:30]\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Feature Importance Top 20')\n",
        "sns.barplot(x=ft_top30, y=ft_top30.index)\n",
        "plt.show()"
      ],
      "id": "A1K4IpE36EOo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_0VewXD6TQb"
      },
      "source": [
        "### ê°ì†Œê³ ê° êµ°ì§‘í™” ë° DecisionTree ì‹œê°í™”"
      ],
      "id": "G_0VewXD6TQb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMeltPMA6kla"
      },
      "source": [
        "month = dataset1_2.columns[dataset1_2.columns.str.contains('month')]\n",
        "dataset1_2.drop(month,axis=1,inplace=True)\n",
        "month1 = dataset2_3.columns[dataset2_3.columns.str.contains('month')]\n",
        "dataset2_3.drop(month1,axis=1,inplace=True)\n",
        "\n",
        "# 4ê¸° ê°ì†Œê³ ê°\n",
        "ê°ì†Œê³ ê° = dataset2_3[dataset2_3['label']==1]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "%matplotlib inline\n",
        "\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        " \n",
        "x = ê°ì†Œê³ ê°[['cat1_p_amount_ê°€ê³µì‹í’ˆ_ratio',\n",
        "'cat1_p_amount_ê¸°íƒ€_ratio',     \n",
        "'cat1_p_amount_ë¯¸ìš©í’ˆ_ratio',      \n",
        "'cat1_p_amount_ìŠ¤í¬ì¸ ë ˆì €_ratio', \n",
        "'cat1_p_amount_ì‹ ì„ ì‹í’ˆ_ratio',    \n",
        "'cat1_p_freq_ê°€ê³µì‹í’ˆ_ratio',       \n",
        "'cat1_p_freq_ê¸°íƒ€_ratio',         \n",
        "'cat1_p_freq_ì‹ ì„ ì‹í’ˆ_ratio',       \n",
        "'cat1_p_freq_ì¼ìƒìš©í’ˆ_ratio',       \n",
        "'max_affiliate',                \n",
        "'half_inde_ratio',              \n",
        "'rate_p_freq_wide',             \n",
        "'cat1_p_amount_ì¸í…Œë¦¬ì–´_wide',      \n",
        "'cat1_p_freq_ìŠ¤í¬ì¸ ë ˆì €_wide',       \n",
        "'cat1_p_freq_ì‹ ì„ ì‹í’ˆ_wide',        \n",
        "'buy_count_A_wide',             \n",
        "'price_ratio_ê³ ê°€',               \n",
        "'price_ratio_ì €ê°€',              \n",
        "'price_ratio_ì¤‘ê°€',               \n",
        "'price_wide_ê³ ê°€',               \n",
        "'price_wide_ì €ê°€',                \n",
        "'cat1_price_ratio_êµìœ¡ë¬¸í™”ê³ ê°€',      \n",
        "'cat1_price_ratio_ê¸°íƒ€ê³ ê°€',        \n",
        "'cat1_price_ratio_ê¸°íƒ€ì¤‘ê°€',        \n",
        "'cat1_price_ratio_ë””ì§€í„¸ê³ ê°€',       \n",
        "'cat1_price_ratio_ë””ì§€í„¸ì €ê°€',       \n",
        "'cat1_price_ratio_ë””ì§€í„¸ì¤‘ê°€',       \n",
        "'cat1_price_ratio_ìŠ¤í¬ì¸ ë ˆì €ì¤‘ê°€',     \n",
        "'cat1_price_ratio_ì¸í…Œë¦¬ì–´ê³ ê°€',      \n",
        "'cat1_price_ratio_íŒ¨ì…˜ì¡í™”ì¤‘ê°€',      \n",
        "'cat1_price_wide_ê°€ê³µì‹í’ˆì¤‘ê°€',       \n",
        "'cat1_price_wide_êµìœ¡ë¬¸í™”ê³ ê°€',       \n",
        "'cat1_price_wide_ê¸°íƒ€ê³ ê°€',         \n",
        "'cat1_price_wide_ë””ì§€í„¸ê³ ê°€',        \n",
        "'cat1_price_wide_ë””ì§€í„¸ì €ê°€',        \n",
        "'cat1_price_wide_ìŠ¤í¬ì¸ ë ˆì €ì €ê°€',      \n",
        "'cat1_price_wide_ì‹ ì„ ì‹í’ˆì¤‘ê°€',       \n",
        "'cat1_price_wide_ì˜ì•½ì €ê°€',         \n",
        "'cat1_price_wide_ì¸í…Œë¦¬ì–´ê³ ê°€',       \n",
        "'cat1_price_wide_ì¸í…Œë¦¬ì–´ì¤‘ê°€',       \n",
        "'cont_ratio_cat1ê°€ê³µì‹í’ˆ',          \n",
        "'cont_ratio_cat1ë””ì§€í„¸',           \n",
        "'cont_ratio_affiliateA',        \n",
        "'class1_p_amount_ì„ ë§¤í’ˆ_ratio',    \n",
        "'class1_p_amount_ì„ ë§¤í’ˆ_wide',     \n",
        "'class1_p_freq_í¸ì˜í’ˆ_wide',       \n",
        "'class1_p_freq_í¸ì˜í’ˆ_net']]\n",
        "\n",
        "\n",
        "def elbow(X):\n",
        "    sse = []\n",
        "\n",
        "    for i in range(1,11):\n",
        "        km = KMeans(n_clusters=i,algorithm='auto', random_state=42)\n",
        "        km.fit(X)\n",
        "        sse.append(km.inertia_)\n",
        "\n",
        "    plt.plot(range(1,11), sse, marker='o')\n",
        "    plt.xlabel('K')\n",
        "    plt.ylabel('SSE')\n",
        "    plt.show()\n",
        "\n",
        "elbow(x)"
      ],
      "id": "BMeltPMA6kla",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfRTymyk7V9f"
      },
      "source": [
        "def visualize_silhouette(cluster_lists, X_features): \n",
        "\n",
        "    from sklearn.datasets import make_blobs\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.cm as cm\n",
        "    import math\n",
        "\n",
        "    # ì…ë ¥ê°’ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ê°¯ìˆ˜ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ì„œ, ê° ê°¯ìˆ˜ë³„ë¡œ í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ê³  ì‹¤ë£¨ì—£ ê°œìˆ˜ë¥¼ êµ¬í•¨\n",
        "    n_cols = len(cluster_lists)\n",
        "\n",
        "    # plt.subplots()ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ì— ê¸°ì¬ëœ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜ë§Œí¼ì˜ sub figuresë¥¼ ê°€ì§€ëŠ” axs ìƒì„± \n",
        "    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)\n",
        "\n",
        "    # ë¦¬ìŠ¤íŠ¸ì— ê¸°ì¬ëœ í´ëŸ¬ìŠ¤í„°ë§ ê°¯ìˆ˜ë“¤ì„ ì°¨ë¡€ë¡œ iteration ìˆ˜í–‰í•˜ë©´ì„œ ì‹¤ë£¨ì—£ ê°œìˆ˜ ì‹œê°í™”\n",
        "    for ind, n_cluster in enumerate(cluster_lists):\n",
        "\n",
        "        # KMeans í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰í•˜ê³ , ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ì™€ ê°œë³„ ë°ì´í„°ì˜ ì‹¤ë£¨ì—£ ê°’ ê³„ì‚°. \n",
        "        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)\n",
        "        cluster_labels = clusterer.fit_predict(X_features)\n",
        "\n",
        "        sil_avg = silhouette_score(X_features, cluster_labels)\n",
        "        sil_values = silhouette_samples(X_features, cluster_labels)\n",
        "\n",
        "        y_lower = 10\n",
        "        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\\n' \\\n",
        "                          'Silhouette Score :' + str(round(sil_avg,3)) )\n",
        "        axs[ind].set_xlabel(\"The silhouette coefficient values\")\n",
        "        axs[ind].set_ylabel(\"Cluster label\")\n",
        "        axs[ind].set_xlim([-0.1, 1])\n",
        "        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])\n",
        "        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks\n",
        "        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "        # í´ëŸ¬ìŠ¤í„°ë§ ê°¯ìˆ˜ë³„ë¡œ fill_betweenx( )í˜•íƒœì˜ ë§‰ëŒ€ ê·¸ë˜í”„ í‘œí˜„. \n",
        "        for i in range(n_cluster):\n",
        "            ith_cluster_sil_values = sil_values[cluster_labels==i]\n",
        "            ith_cluster_sil_values.sort()\n",
        "\n",
        "            size_cluster_i = ith_cluster_sil_values.shape[0]\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = cm.nipy_spectral(float(i) / n_cluster)\n",
        "            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \\\n",
        "                                facecolor=color, edgecolor=color, alpha=0.7)\n",
        "            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "            y_lower = y_upper + 10\n",
        "\n",
        "        axs[ind].axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "# make_blobs ì„ í†µí•´ clustering ì„ ìœ„í•œ 4ê°œì˜ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì˜ 500ê°œ 2ì°¨ì› ë°ì´í„° ì…‹ ìƒì„±  \n",
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, \\\n",
        "                  center_box=(-10.0, 10.0), shuffle=True, random_state=1)  \n",
        "\n",
        "# cluster ê°œìˆ˜ë¥¼ 2ê°œ, 3ê°œ, 4ê°œ, 5ê°œ ì¼ë•Œì˜ í´ëŸ¬ìŠ¤í„°ë³„ ì‹¤ë£¨ì—£ ê³„ìˆ˜ í‰ê· ê°’ì„ ì‹œê°í™” \n",
        "visualize_silhouette([ 2, 3, 4, 5], X)"
      ],
      "id": "NfRTymyk7V9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5szxTbC67Zn"
      },
      "source": [
        "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, random_state=0)\n",
        "kmeans.fit(x)\n",
        "ê°ì†Œê³ ê°['cluster']=kmeans.labels_"
      ],
      "id": "W5szxTbC67Zn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiq3PAfl6amd"
      },
      "source": [
        "group0 = ê°ì†Œê³ ê°[ê°ì†Œê³ ê°['cluster']==0]\n",
        "group1 = ê°ì†Œê³ ê°[ê°ì†Œê³ ê°['cluster']==1]\n",
        "group2 = ê°ì†Œê³ ê°[ê°ì†Œê³ ê°['cluster']==2]\n",
        "group3 = ê°ì†Œê³ ê°[ê°ì†Œê³ ê°['cluster']==3]"
      ],
      "id": "xiq3PAfl6amd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slj0ajc47D81"
      },
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "export_graphviz(dt_clf, out_file='tree.dot',class_names=['0','1','2','3'], feature_names=['cat1_p_amount_ê°€ê³µì‹í’ˆ_ratio',\n",
        "'cat1_p_amount_ê¸°íƒ€_ratio',     \n",
        "'cat1_p_amount_ë¯¸ìš©í’ˆ_ratio',      \n",
        "'cat1_p_amount_ìŠ¤í¬ì¸ ë ˆì €_ratio', \n",
        "'cat1_p_amount_ì‹ ì„ ì‹í’ˆ_ratio',    \n",
        "'cat1_p_freq_ê°€ê³µì‹í’ˆ_ratio',       \n",
        "'cat1_p_freq_ê¸°íƒ€_ratio',         \n",
        "'cat1_p_freq_ì‹ ì„ ì‹í’ˆ_ratio',       \n",
        "'cat1_p_freq_ì¼ìƒìš©í’ˆ_ratio',       \n",
        "'max_affiliate',                \n",
        "'half_inde_ratio',              \n",
        "'rate_p_freq_wide',             \n",
        "'cat1_p_amount_ì¸í…Œë¦¬ì–´_wide',      \n",
        "'cat1_p_freq_ìŠ¤í¬ì¸ ë ˆì €_wide',       \n",
        "'cat1_p_freq_ì‹ ì„ ì‹í’ˆ_wide',        \n",
        "'buy_count_A_wide',             \n",
        "'price_ratio_ê³ ê°€',               \n",
        "'price_ratio_ì €ê°€',              \n",
        "'price_ratio_ì¤‘ê°€',               \n",
        "'price_wide_ê³ ê°€',               \n",
        "'price_wide_ì €ê°€',                \n",
        "'cat1_price_ratio_êµìœ¡ë¬¸í™”ê³ ê°€',      \n",
        "'cat1_price_ratio_ê¸°íƒ€ê³ ê°€',        \n",
        "'cat1_price_ratio_ê¸°íƒ€ì¤‘ê°€',        \n",
        "'cat1_price_ratio_ë””ì§€í„¸ê³ ê°€',       \n",
        "'cat1_price_ratio_ë””ì§€í„¸ì €ê°€',       \n",
        "'cat1_price_ratio_ë””ì§€í„¸ì¤‘ê°€',       \n",
        "'cat1_price_ratio_ìŠ¤í¬ì¸ ë ˆì €ì¤‘ê°€',     \n",
        "'cat1_price_ratio_ì¸í…Œë¦¬ì–´ê³ ê°€',      \n",
        "'cat1_price_ratio_íŒ¨ì…˜ì¡í™”ì¤‘ê°€',      \n",
        "'cat1_price_wide_ê°€ê³µì‹í’ˆì¤‘ê°€',       \n",
        "'cat1_price_wide_êµìœ¡ë¬¸í™”ê³ ê°€',       \n",
        "'cat1_price_wide_ê¸°íƒ€ê³ ê°€',         \n",
        "'cat1_price_wide_ë””ì§€í„¸ê³ ê°€',        \n",
        "'cat1_price_wide_ë””ì§€í„¸ì €ê°€',        \n",
        "'cat1_price_wide_ìŠ¤í¬ì¸ ë ˆì €ì €ê°€',      \n",
        "'cat1_price_wide_ì‹ ì„ ì‹í’ˆì¤‘ê°€',       \n",
        "'cat1_price_wide_ì˜ì•½ì €ê°€',         \n",
        "'cat1_price_wide_ì¸í…Œë¦¬ì–´ê³ ê°€',       \n",
        "'cat1_price_wide_ì¸í…Œë¦¬ì–´ì¤‘ê°€',       \n",
        "'cont_ratio_cat1ê°€ê³µì‹í’ˆ',          \n",
        "'cont_ratio_cat1ë””ì§€í„¸',           \n",
        "'cont_ratio_affiliateA',        \n",
        "'class1_p_amount_ì„ ë§¤í’ˆ_ratio',    \n",
        "'class1_p_amount_ì„ ë§¤í’ˆ_wide',     \n",
        "'class1_p_freq_í¸ì˜í’ˆ_wide',       \n",
        "'class1_p_freq_í¸ì˜í’ˆ_net'], impurity=True, filled=True)"
      ],
      "id": "slj0ajc47D81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GbiRoU77JGs"
      },
      "source": [
        "import graphviz\n",
        "\n",
        "with  open('tree.dot',encoding='UTF8') as f:\n",
        "    dot_graph = f.read()\n",
        "graphviz.Source(dot_graph)"
      ],
      "id": "0GbiRoU77JGs",
      "execution_count": null,
      "outputs": []
    }
  ]
}